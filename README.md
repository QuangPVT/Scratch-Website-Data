---

# Scratch-Website-Data

## Overview

This project involves scraping data from a real estate website in Ho Chi Minh City, Vietnam, using Python and the Selenium library. The goal is to collect data about real estate listings and save this information for further analysis or use in other applications.

The project is organized into two main directories:
1. **Code**
2. **Data**

### Code

The `Code` directory contains all the scripts and related resources necessary to execute the web scraping process. The primary components of this directory include:

- **`B1_Get_Links.py`**: This script is responsible for collecting the URLs of the real estate listings from the target website. It navigates through the website's pages and extracts the links for further processing.
  
- **`B2_Data_Scraper.py`**: This script uses the URLs obtained from `B1_Get_Links.py` to scrape detailed information about each property listing. This includes details like property type, price, location, and other relevant data.

- **`B3_Clean_Data.py`**: This script processes the raw data scraped by `B2_Data_Scraper.py`, cleaning and structuring it for easier analysis. It handles missing data, formats the data consistently, and prepares it for storage.

- **`Group_Data.py`**: This script consolidates the cleaned data from multiple runs into a single dataset. It combines the data files and ensures that all relevant information is included in the final dataset.

- **`README.txt`**: A text file providing a brief overview of each script and its purpose.

- **`requirements.txt`**: A file listing all the Python libraries and dependencies needed to run the scraping scripts. You can install these dependencies using pip by running `pip install -r requirements.txt`.

### Data

The `Data` directory is where all the collected data is stored. This directory is organized into the following subdirectories:

- **`data-hcm-links`**: Contains the raw URL links extracted from the website by `B1_Get_Links.py`.

- **`data-hcm-csv`**: Contains the raw data files in CSV format, scraped using `B2_Data_Scraper.py`.

- **`data-hcm-cleaned`**: Contains the cleaned and processed data, structured for analysis, generated by `B3_Clean_Data.py`.

## Getting Started

### Prerequisites

To run this project, you need to have the following installed:

- Python 3.9.13
- Google Chrome Browser (or another supported browser)
- ChromeDriver (or the equivalent driver for your browser)
- The necessary Python libraries, which can be installed using the provided `requirements.txt` file.

### How to Run

1. Clone this repository to your local machine.
2. Navigate to the project directory.
3. Install the required Python libraries:
   ```sh
   pip install -r requirements.txt
   ```
4. Run the scripts in the following order:
   - `B1_Get_Links.py` to gather the links.
   - `B2_Data_Scraper.py` to scrape the property details.
   - `B3_Clean_Data.py` to clean and structure the data.
   - `Group_Data.py` to consolidate the cleaned data into a single dataset.
5. The processed data will be saved in the `Data/data-hcm-cleaned` directory.
```
